# ğŸ§  ToxicCheck - Toxic Comment Classifier

ToxicCheck is a web-based tool for classifying and visualizing the toxicity of user-generated comments. It uses a trained deep learning model and a custom text vectorizer to detect toxic language and display severity using an interactive UI.

![ToxicCheck Screenshot](docs/demo_screenshot.png) <!-- Add your own screenshot if available -->

---

## ğŸš€ Features

- ğŸ” **Real-Time Analysis** â€” Detect toxicity in seconds  
- ğŸ“Š **Toxicity Meter** â€” See severity from *Safe* to *Extreme*  
- ğŸ§  **Multi-Label Detection** â€” Identifies types like `toxic`, `insult`, `obscene`, etc.  
- ğŸ’¡ **Dynamic UI** â€” Clean, responsive layout with animation and badges  
- ğŸ” **Secure Backend** â€” Django-powered with CSRF protection

---

## ğŸ› ï¸ Tech Stack

| Component     | Tech                       |
|---------------|----------------------------|
| Frontend      | HTML, CSS, JavaScript      |
| Backend       | Django, Django REST        |
| ML Model      | TensorFlow/Keras           |
| Vectorization | Custom `TfidfVectorizer`   |
| Format        | Model (`.h5`), Vectorizer (`.pkl`) |

---

